1045. Customers Who Bought All Products
Table: Customer

+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| customer_id | int     |
| product_key | int     |
+-------------+---------+

This table may contain duplicates rows. 
customer_id is not NULL.
product_key is a foreign key (reference column) to Product table.
 

Table: Product

+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_key | int     |
+-------------+---------+

product_key is the primary key (column with unique values) for this table.

Write a solution to report the customer ids from the Customer table 
that bought all the products in the Product table.

Return the result table in any order.

The result format is in the following example.

Example 1:

Input: 
Customer table:
+-------------+-------------+
| customer_id | product_key |
+-------------+-------------+
| 1           | 5           |
| 2           | 6           |
| 3           | 5           |
| 3           | 6           |
| 1           | 6           |
+-------------+-------------+

Product table:
+-------------+
| product_key |
+-------------+
| 5           |
| 6           |
+-------------+

Output: 
+-------------+
| customer_id |
+-------------+
| 1           |
| 3           |
+-------------+

Explanation: 
The customers who bought all the products (5 and 6) are customers with IDs 1 and 3.

----------------------------

SELECT customer_id
FROM Customer
WHERE product_key IN (SELECT product_key FROM Product)
GROUP BY customer_id
HAVING COUNT(DISTINCT product_key) = (SELECT COUNT(*) FROM Product);

---------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, countDistinct

customer_df = spark.createDataFrame([
    (1, 5),
    (2, 6),
    (3, 5),
    (3, 6),
    (1, 6),
], ["customer_id", "product_key"])

product_df = spark.createDataFrame([
    (5,),
    (6,)
], ["product_key"])

total_products = product_df.select("product_key").distinct().count()

valid_purchases = customer_df.join(product_df, on="product_key", how="inner")

customer_product_counts = (
    valid_purchases
    .groupBy("customer_id")
    .agg(countDistinct("product_key").alias("product_count"))
)

customers_who_bought_all = customer_product_counts.filter(
    col("product_count") == total_products
).select("customer_id")

customers_who_bought_all.show()
---------------------------------
#we can also do same like sql IN but join Wins ebcause:
valid_product_keys = product_df.select("product_key").rdd.flatMap(lambda x: x).collect()

filtered_customer_df = customer_df.filter(col("product_key").isin(valid_product_keys))

However, here's why the join wins in PySpark:
- It's more scalable for large datasets — avoids collecting data to the driver.
- Leverages Spark’s distributed engine efficiently.
- Ensures tight schema alignment and lets you pull in additional fields if needed
