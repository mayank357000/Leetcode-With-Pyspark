534. Game Play Analysis III
Best video game consoles
Table: Activity

+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| player_id    | int     |
| device_id    | int     |
| event_date   | date    |
| games_played | int     |
+--------------+---------+
(player_id, event_date) is the primary key of this table.
This table shows the activity of players of some game.
Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on some day using some device.

Write an SQL query that reports for each player and date, how many games played so far by the player. That is, the total number of games played by the player until that date. Check the example for clarity.

The query result format is in the following example:

Activity table:
+-----------+-----------+------------+--------------+
| player_id | device_id | event_date | games_played |
+-----------+-----------+------------+--------------+
| 1         | 2         | 2016-03-01 | 5            |
| 1         | 2         | 2016-05-02 | 6            |
| 1         | 3         | 2017-06-25 | 1            |
| 3         | 1         | 2016-03-02 | 0            |
| 3         | 4         | 2018-07-03 | 5            |
+-----------+-----------+------------+--------------+

Result table:
+-----------+------------+---------------------+
| player_id | event_date | games_played_so_far |
+-----------+------------+---------------------+
| 1         | 2016-03-01 | 5                   |
| 1         | 2016-05-02 | 11                  |
| 1         | 2017-06-25 | 12                  |
| 3         | 2016-03-02 | 0                   |
| 3         | 2018-07-03 | 5                   |
+-----------+------------+---------------------+
For the player with id 1, 5 + 6 = 11 games played by 2016-05-02, and 5 + 6 + 1 = 12 games played by 2017-06-25.
For the player with id 3, 0 + 5 = 5 games played by 2018-07-03.
Note that for each player we only care about the days when the player logged in.

-------------------------------
SELECT 
    player_id,
    event_date,
    SUM(games_played) OVER (
        PARTITION BY player_id 
        ORDER BY event_date
        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) AS games_played_so_far
FROM Activity;

---------------------
from pyspark.sql import functions as F
from pyspark.sql.window import Window

window_spec = Window.partitionBy("player_id") \
    .orderBy("event_date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

result = activity_df.withColumn(
    "games_played_so_far",
    F.sum("games_played").over(window_spec)
).select(
    "player_id", "event_date", "games_played_so_far"
)

------------------------
For rolling sum we can do one more thing
either self join and for each record we can get records mathced less than date to it 
and then do sum

or similarly we can do corelated subquery where we for each record will match rows and sum
(pyspakr don't support corelated subqueries so only for sql this way)
------------------
SELECT a.player_id, a.event_date,
       SUM(b.games_played) AS games_played_so_far
FROM Activity a
JOIN Activity b
  ON a.player_id = b.player_id
 AND b.event_date <= a.event_date
GROUP BY a.player_id, a.event_date;
----------
SELECT a.player_id, a.event_date,
       (
         SELECT SUM(b.games_played)
         FROM Activity b
         WHERE b.player_id = a.player_id
           AND b.event_date <= a.event_date
       ) AS games_played_so_far
FROM Activity a;
----------------

That’s right — PySpark doesn't support correlated subqueries in the traditional SQL sense because it operates on distributed DataFrame transformations, not on row-by-row evaluation or scalar subqueries tied to outer queries.
Correlated subqueries rely on evaluating a subquery once per row of the outer query. In PySpark, this concept doesn’t map cleanly due to parallelism — instead, you'd simulate it using:
- self joins, as we did earlier
- window functions, when possible
- broadcast joins, if one side is small enough
- groupBy with filters, for aggregation-style logic
