1113. Reported Posts
Table: Actions

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| user_id       | int     |
| post_id       | int     |
| action_date   | date    |
| action        | enum    |
| extra         | varchar |
+---------------+---------+

The extra column has optional information about the action such as a reason for report or a type of reaction. 

Write an SQL query that reports the number of posts reported yesterday for each report reason. Assume today is 2019-07-05.

The query result format is in the following example:

Actions table:
+---------+---------+-------------+--------+--------+
| user_id | post_id | action_date | action | extra  |
+---------+---------+-------------+--------+--------+
| 1       | 1       | 2019-07-01  | view   | null   |
| 1       | 1       | 2019-07-01  | like   | null   |
| 1       | 1       | 2019-07-01  | share  | null   |
| 2       | 4       | 2019-07-04  | view   | null   |
| 2       | 4       | 2019-07-04  | report | spam   |
| 3       | 4       | 2019-07-04  | view   | null   |
| 3       | 4       | 2019-07-04  | report | spam   |
| 4       | 3       | 2019-07-02  | view   | null   |
| 4       | 3       | 2019-07-02  | report | spam   |
| 5       | 2       | 2019-07-04  | view   | null   |
| 5       | 2       | 2019-07-04  | report | racism |
| 5       | 5       | 2019-07-04  | view   | null   |
| 5       | 5       | 2019-07-04  | report | racism |
+---------+---------+-------------+--------+--------+

Result table:
+---------------+--------------+
| report_reason | report_count |
+---------------+--------------+
| spam          | 1            |
| racism        | 2            |
+---------------+--------------+
Note that we only care about report reasons with non zero number of reports.

-------------------
select 
    extra as report_reason,count(distinct post_id) 
    from actions 
    where action_date='2019-07-04'
    and action ='report'
    and extra is not null 
group by extra;

----------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, countDistinct

# Initialize Spark session
spark = SparkSession.builder.appName("IndustryReadyPySpark").getOrCreate()

# Load the data (assuming it's already in a DataFrame format)
actions_df = spark.read.parquet("path/to/actions_data.parquet")  # Adjust path accordingly

#isNotNull() function can be used on col() object

result_df=actions_df.filter(
        (col("action_date")=="2019-07-04")&
        (col("action")=="report")&
        (col("extra").isNotNull())
    )\
    .groupBy("extra")\ 
    .agg(countDistinct("post_id").alias("report_count"))\
    .withColumnRenamed("extra","report_reason")

result_df.show()

All expressions and transformations in PySpark 
(like col("name"), when(...), countDistinct(...), etc.) 
return a Column object, we can use alias on it
ex: df.select((col("a") + col("b")).alias("sum"))