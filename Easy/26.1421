1421. NPV Queries
SQL Schema 
Table: NPV

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| year          | int     |
| npv           | int     |
+---------------+---------+
(id, year) is the primary key of this table.
The table has information about the id and the year of each inventory and the corresponding net present value.
 

Table: Queries

+---------------+---------+
| Column Name   | Type    |
+---------------+---------+
| id            | int     |
| year          | int     |
+---------------+---------+
(id, year) is the primary key of this table.
The table has information about the id and the year of each inventory query.
 

Write an SQL query to find the npv of all each query of queries table.

Return the result table in any order.

The query result format is in the following example:

NPV table:
+------+--------+--------+
| id   | year   | npv    |
+------+--------+--------+
| 1    | 2018   | 100    |
| 7    | 2020   | 30     |
| 13   | 2019   | 40     |
| 1    | 2019   | 113    |
| 2    | 2008   | 121    |
| 3    | 2009   | 12     |
| 11   | 2020   | 99     |
| 7    | 2019   | 0      |
+------+--------+--------+

Queries table:
+------+--------+
| id   | year   |
+------+--------+
| 1    | 2019   |
| 2    | 2008   |
| 3    | 2009   |
| 7    | 2018   |
| 7    | 2019   |
| 7    | 2020   |
| 13   | 2019   |
+------+--------+

Result table:
+------+--------+--------+
| id   | year   | npv    |
+------+--------+--------+
| 1    | 2019   | 113    |
| 2    | 2008   | 121    |
| 3    | 2009   | 12     |
| 7    | 2018   | 0      |
| 7    | 2019   | 0      |
| 7    | 2020   | 30     |
| 13   | 2019   | 40     |
+------+--------+--------+

The npv value of (7, 2018) is not present in the NPV table, we consider it 0.
The npv values of all other queries can be found in the NPV table.

-----------------------------------------------------------------

SELECT 
    q.id, 
    q.year, 
    CASE 
        WHEN n.npv IS NULL THEN 0 
        ELSE n.npv 
    END AS npv
FROM Queries q
LEFT JOIN NPV n
    ON q.id = n.id AND q.year = n.year;

or

 COALESCE(n.npv, 0) AS npv
-------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, coalesce, lit

spark = SparkSession.builder.appName("NPVQueries").getOrCreate()

npv_df = spark.read.csv("NPV.csv", header=True, inferSchema=True)
queries_df = spark.read.csv("Queries.csv", header=True, inferSchema=True)

#df.printSchema()
#df.show()

joined_df = queries_df.join(
    npv_df,
    on=[queries_df.id == npv_df.id, queries_df.year == npv_df.year],
    how="left"
)

#or use (cond1)&(cond2) as second parameter

#lit retunrs a col() obejct so can be used in withColumn

joined_df = joined_df.withColumn("npv", coalesce(npv_df.npv, lit(0)))

result_df = joined_df.select(queries_df.id, queries_df.year, col("npv"))
result_df.show()

----------------
#if want to mention schema explicitly

from pyspark.sql.types import StructType, StructField, IntegerType

#StructType takes a input of list of StructField Objects
npv_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("year", IntegerType(), True),
    StructField("npv", IntegerType(), True)
])

queries_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("year", IntegerType(), True)
])

npv_df = spark.read.csv("NPV.csv", header=True, schema=npv_schema)
queries_df = spark.read.csv("Queries.csv", header=True, schema=queries_schema)

------------------------
#else can cast wrong cols if they were made to string
from pyspark.sql.functions import col

#in sql it is CAST(COL_NAME AS INT/FLOAT/DOUBLE/STRING)

#.cast("") can be used on col() object
npv_df = npv_df.withColumn("id", col("id").cast("int")) \
               .withColumn("year", col("year").cast("int")/cast("date")/cast("timestamp")) \
               .withColumn("npv", col("npv").cast("int"))

queries_df = queries_df.withColumn("id", col("id").cast("int")) \
                       .withColumn("year", col("year").cast("int"))

-------------------------
If your CSV does not have a header, set header=False and provide column names using .toDF():

df = spark.read.csv("file.csv", header=False), _c1,_c2,_c3.... default headers
df = df.toDF("id", "year", "npv")

#.toDF() is used to rename all columns of a DataFrame at once, drop extra col if present
#for smaller set of col use withColumnRenamed to do this
#can also convert rdd to df , while doing so it can take column names or not
rdd = spark.sparkContext.parallelize([(1, 2020, 100), (2, 2021, 200)])
df1 = rdd.toDF()  # Columns: _1, _2, _3
df2 = rdd.toDF("id", "year", "npv")  # Columns: id, year, npv